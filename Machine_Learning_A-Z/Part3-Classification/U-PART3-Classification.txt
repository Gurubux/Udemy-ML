**********************************************************************************************
Part 3: Classification
**********************************************************************************************
**********************************************************************************************
Section 12: Logistic Regression
**********************************************************************************************
from sklearn.linear_model import LogisticRegression
classifier = LogisticRegression(random_state = 0)
classifier.fit(X_train, y_train)

**********************************************************************************************
Section 13: K-Nearest Neighbors (K-NN)
**********************************************************************************************
from sklearn.neighbors import KNeighborsClassifier
classifier = KNeighborsClassifier(n_neighbors = 5, metric = 'minkowski', p = 2)
classifier.fit(X_train, y_train)

**********************************************************************************************
Section 14: Support Vector Machine (SVM)
**********************************************************************************************
from sklearn.svm import SVC
classifier = SVC(kernel = 'linear', random_state = 0)
classifier.fit(X_train, y_train)


**********************************************************************************************
Section 15: Kernel SVM
**********************************************************************************************
from sklearn.svm import SVC
classifier = SVC(kernel = 'rbf', random_state = 0)
classifier.fit(X_train, y_train)


**********************************************************************************************
Section 16: Naive Bayes
**********************************************************************************************
from sklearn.naive_bayes import GaussianNB
classifier = GaussianNB()
classifier.fit(X_train, y_train)


**********************************************************************************************
Section 17: Decision Tree Classification
**********************************************************************************************
from sklearn.tree import DecisionTreeClassifier
classifier = DecisionTreeClassifier(criterion = 'entropy', random_state = 0)
classifier.fit(X_train, y_train)



**********************************************************************************************
Section 18: Random Forest Classification
**********************************************************************************************
from sklearn.ensemble import RandomForestClassifier
classifier = RandomForestClassifier(n_estimators = 10, criterion = 'entropy', random_state = 0)
classifier.fit(X_train, y_train)


**********************************************************************************************
Section 19: Evaluating Classification Models Performance
**********************************************************************************************

Classification Model			Pros	Cons
Logistic 						Regression	Probabilistic approach, gives informations about statistical significance of features	The Logistic Regression Assumptions
K-NN 							Simple to understand, fast and efficient 	Need to choose the number of neighbours k
SVM								Performant, not biased by outliers, not sensitive to overfitting	Not appropriate for non linear problems, not the best choice for large number of features
Kernel SVM						High performance on nonlinear problems, not biased by outliers, not sensitive to overfitting	Not the best choice for large number of features, more complex
Naive Bayes						Efficient, not biased by outliers, works on nonlinear problems, probabilistic approach	Based on the assumption that features have same statistical relevance
Decision Tree Classification	Interpretability, no need for feature scaling, works on both linear / nonlinear problems	Poor results on too small datasets, overfitting can easily occur
Random Forest Classification	Powerful and accurate, good performance on many problems, including non linear	No interpretability, overfitting can easily occur, need to choose the number of trees

