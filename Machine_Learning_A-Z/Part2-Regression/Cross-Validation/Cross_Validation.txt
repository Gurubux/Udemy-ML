Cross-Validation allows us to compare different machine learning methods and get a sense of how well they will work in practice.

Training Set and testing set are divided randomly- To choose the best among each for testing is a overhead, thus Cross-validation uses themm all, one at a time, and summarizes the results at the end.
Example : Consider 4 blocks of data : Total Data : |--1-25%--|--2-25%--|--3-25%--|--4-25%--|
			1. Cross-validation will start by using the 1st three blocks to train the model and Last block to test the model
			2. Keeps track how well the model did, say test data had 5 - Correct and 1 - InCorrect answer
			3. Repeat all combinations and keep track and then which ever ML Algo(SVM, Logistic, DT, RF etc.) gives best result for all test blocks combined that is chosen.
		Since we divided data in 4 blocks this is called as 'FOUR-FOLD CROSS VALIDATION'
		We can consider each sample as a test data, 'LEAVE ONE OUT CROSS VALIDATION'
		TEN-FOLD CROSS VALIDATION is a common practice

#https://github.com/Gurubux/Udemy-ML/blob/master/Machine_Learning_A-Z/Part2-Regression/Cross-Validation/Cross_Validation_Practice.ipynb
rf_class = RandomForestClassifier(n_estimators=10)
log_class = LogisticRegression()
svm_class = svm.SVC()
from sklearn.model_selection import cross_val_score
print(cross_val_score(rf_class, data_input, data_output, scoring='accuracy', cv = 10))
# [ 1.          0.93333333  1.          0.93333333  0.93333333  0.93333333 	 0.86666667  1.          1.          1.        ]

print("Random Forests: ")
print(cross_val_score(rf_class, data_input, data_output, scoring='accuracy', cv = 10))
accuracy = cross_val_score(rf_class, data_input, data_output, scoring='accuracy', cv = 10).mean() * 100
print("Accuracy of Random Forests is: " , accuracy)
 
print("\n\nSVM:")
print(cross_val_score(svm_class, data_input, data_output, scoring='accuracy', cv = 10))
accuracy = cross_val_score(svm_class, data_input, data_output, scoring='accuracy', cv = 10).mean() * 100
print("Accuracy of SVM is: " , accuracy)
 
print("\n\nLogisticRegression:")
print(cross_val_score(log_class, data_input, data_output, scoring='accuracy', cv = 10))
accuracy = cross_val_score(log_class, data_input, data_output, scoring='accuracy', cv = 10).mean() * 100
print("Accuracy of SVM is: " , accuracy)


"""
Random Forests: 
[ 1.          0.93333333  1.          0.93333333  0.93333333  0.93333333
  0.86666667  1.          1.          1.        ]
Accuracy of Random Forests is:  95.3333333333
 
 
SVM:
[ 1.          0.93333333  1.          1.          1.          0.93333333
  0.93333333  1.          1.          1.        ]
Accuracy of SVM is:  98.0
 
 
Log:
[ 1.          1.          1.          0.93333333  0.93333333  0.93333333
  0.8         0.93333333  1.          1.        ]
Accuracy of SVM is:  95.3333333333
"""

Few types of evaluation approaches that can be used to achieve this goal.These approaches are: 
	1. train and test on the same dataset, and 
	2. train/test split.
	3. K Fold 					
		cross_val_score()
		cross_val_score().mean()
	4. ShuffleSplit()
		cross_val_score(rf_class, data_input, data_output, scoring='accuracy', cv = 10)
		cv = ShuffleSplit(n_splits=100, test_size=0.2, random_state=0)
		cross_val_score(rf_class, data_input, data_output, scoring='accuracy', cv = 10)
		accuracy = cross_val_score(rf_class, data_input, data_output, scoring='accuracy', cv = 10).mean() * 100
		print("Accuracy of SVM is: " , accuracy)



https://github.com/Gurubux/Udemy-ML/blob/master/Machine_Learning_A-Z/Part2-Regression/Cross-Validation/07_cross_validation.ipynb