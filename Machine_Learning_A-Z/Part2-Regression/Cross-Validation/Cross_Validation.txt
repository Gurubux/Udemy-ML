Cross-Validation
------------------------------------------------------------------------------------------------------
Classification and Regression
------------------------------------------------------------------------------------------------------
Cross-Validation allows us to compare different machine learning methods and get a sense of how well they will work in practice.

Training Set and testing set are divided randomly- To choose the best among each for testing is a overhead, thus Cross-validation uses themm all, one at a time, and summarizes the results at the end.
Example : Consider 4 blocks of data : Total Data : |--1-25%--|--2-25%--|--3-25%--|--4-25%--|
			1. Cross-validation will start by using the 1st three blocks to train the model and Last block to test the model
			2. Keeps track how well the model did, say test data had 5 - Correct and 1 - InCorrect answer
			3. Repeat all combinations and keep track and then which ever ML Algo(SVM, Logistic, DT, RF etc.) gives best result for all test blocks combined that is chosen.
		Since we divided data in 4 blocks this is called as 'FOUR-FOLD CROSS VALIDATION'
		We can consider each sample as a test data, 'LEAVE ONE OUT CROSS VALIDATION'
		TEN-FOLD CROSS VALIDATION is a common practice
------------------------------------------------------------------------------------------------------------------------------------
#https://github.com/Gurubux/Udemy-ML/blob/master/Machine_Learning_A-Z/Part2-Regression/Cross-Validation/Cross_Validation_Practice.ipynb
rf_class = RandomForestClassifier(n_estimators=10)
log_class = LogisticRegression()
svm_class = svm.SVC()
from sklearn.model_selection import cross_val_score
print(cross_val_score(rf_class, data_input, data_output, scoring='accuracy', cv = 10))
# [ 1.          0.93333333  1.          0.93333333  0.93333333  0.93333333 	 0.86666667  1.          1.          1.        ]

print("Random Forests: ")
print(cross_val_score(rf_class, data_input, data_output, scoring='accuracy', cv = 10))
accuracy = cross_val_score(rf_class, data_input, data_output, scoring='accuracy', cv = 10).mean() * 100
print("Accuracy of Random Forests is: " , accuracy)
 
print("\n\nSVM:")
print(cross_val_score(svm_class, data_input, data_output, scoring='accuracy', cv = 10))
accuracy = cross_val_score(svm_class, data_input, data_output, scoring='accuracy', cv = 10).mean() * 100
print("Accuracy of SVM is: " , accuracy)
 
print("\n\nLogisticRegression:")
print(cross_val_score(log_class, data_input, data_output, scoring='accuracy', cv = 10))
accuracy = cross_val_score(log_class, data_input, data_output, scoring='accuracy', cv = 10).mean() * 100
print("Accuracy of SVM is: " , accuracy)


"""
Random Forests: 
[ 1.          0.93333333  1.          0.93333333  0.93333333  0.93333333
  0.86666667  1.          1.          1.        ]
Accuracy of Random Forests is:  95.3333333333
 
 
SVM:
[ 1.          0.93333333  1.          1.          1.          0.93333333
  0.93333333  1.          1.          1.        ]
Accuracy of SVM is:  98.0
 
 
Log:
[ 1.          1.          1.          0.93333333  0.93333333  0.93333333
  0.8         0.93333333  1.          1.        ]
Accuracy of SVM is:  95.3333333333
"""
------------------------------------------------------------------------------------------------------------------------------------
#https://github.com/Gurubux/Data-Lit/blob/master/2-StatisticsAndProbability/2.1-CreditScoring/Loan_Default_Prediction.ipynb
Few types of evaluation approaches that can be used to achieve this goal.These approaches are: 
	1. train and test on the same dataset, and 
	2. train/test split.
	3. K Fold 					
		cross_val_score()
		cross_val_score().mean()
	4. ShuffleSplit()
		cross_val_score(rf_class, data_input, data_output, scoring='accuracy', cv = 10)
		OR

		cv = ShuffleSplit(n_splits=100, test_size=0.2, random_state=0)
		
		cross_val_score(rf_class, data_input, data_output, scoring='accuracy', cv = 10)
		accuracy = cross_val_score(rf_class, data_input, data_output, scoring='accuracy', cv = 10).mean() * 100
		print("Accuracy of SVM is: " , accuracy)

------------------------------------------------------------------------------------------------------------------------------------
------------------------------------------------------------------------------------------------------------------------------------
------------------------------------------------------------------------------------------------------------------------------------
#https://github.com/Gurubux/Udemy-ML/blob/master/Machine_Learning_A-Z/Part2-Regression/Cross-Validation/07_cross_validation.ipynb
train/test split - High Variance Result
Question: What if we created a bunch of train/test splits, calculated the testing accuracy for each, and averaged the results together?
Answer: That`s the essense of cross-validation!
	https://raw.githubusercontent.com/Gurubux/Udemy-ML/master/Machine_Learning_A-Z/Part2-Regression/Cross-Validation/07_cross_validation_diagram.png

# 5-fold cross-validation:
# simulate splitting a dataset of 25 observations into 5 folds
from sklearn.model_selection import KFold
kf = KFold(n_splits=5, shuffle=False).split(range(25))

# print the contents of each training and testing set
print('{} {:^61} {}'.format('Iteration', 'Training set observations', 'Testing set observations'))
for iteration, data in enumerate(kf, start=1):
    print('{:^9} {} {:^25}'.format(iteration, data[0], str(data[1])))
>>>
		Iteration                   Training set observations                   Testing set observations
		    1     [ 5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24]        [0 1 2 3 4]       
		    2     [ 0  1  2  3  4 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24]        [5 6 7 8 9]       
		    3     [ 0  1  2  3  4  5  6  7  8  9 15 16 17 18 19 20 21 22 23 24]     [10 11 12 13 14]     
		    4     [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 20 21 22 23 24]     [15 16 17 18 19]     
		    5     [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19]     [20 21 22 23 24]     

------------------------------------------------------------------------------------------------------------------------------------
CROSS-VALIDATION EXAMPLE: PARAMETER TUNING
	# search for an optimal value of K for KNN
	k_range = list(range(1, 31))
	k_scores = []
	for k in k_range:
	    knn = KNeighborsClassifier(n_neighbors=k)
	    scores = cross_val_score(knn, X, y, cv=10, scoring='accuracy')
	    k_scores.append(scores.mean())
	print(k_scores)
	# plot the value of K for KNN (x-axis) versus the cross-validated accuracy (y-axis)
	plt.plot(k_range, k_scores)
	plt.xlabel('Value of K for KNN')
	plt.ylabel('Cross-Validated Accuracy')

------------------------------------------------------------------------------------------------------------------------------------
CROSS-VALIDATION EXAMPLE: MODEL SELECTION
	# 10-fold cross-validation with the best KNN model
	knn = KNeighborsClassifier(n_neighbors=20)
	print(cross_val_score(knn, X, y, cv=10, scoring='accuracy').mean())

	# 10-fold cross-validation with logistic regression
	from sklearn.linear_model import LogisticRegression
	logreg = LogisticRegression()
	print(cross_val_score(logreg, X, y, cv=10, scoring='accuracy').mean())

------------------------------------------------------------------------------------------------------------------------------------
CROSS-VALIDATION EXAMPLE: FEATURE SELECTION
	X = data[['TV', 'Radio', 'Newspaper']]
	lm = LinearRegression()
	scores = cross_val_score(lm, X, y, cv=10, scoring='neg_mean_squared_error')
	mse_scores = -scores
	rmse_scores = np.sqrt(mse_scores)

	X = data[['TV', 'Radio']]
	rmse_scores = np.sqrt(-cross_val_score(lm, X, y, cv=10, scoring='neg_mean_squared_error')).mean())


Improvements to cross-validation
Repeated cross-validation

Repeat cross-validation multiple times (with different random splits of the data) and average the results
More reliable estimate of out-of-sample performance by reducing the variance associated with a single trial of cross-validation
Creating a hold-out set

"Hold out" a portion of the data before beginning the model building process
Locate the best model using cross-validation on the remaining data, and test it using the hold-out set
More reliable estimate of out-of-sample performance since hold-out set is truly out-of-sample
Feature engineering and selection within cross-validation iterations

Normally, feature engineering and selection occurs before cross-validation
Instead, perform all feature engineering and selection within each cross-validation iteration
More reliable estimate of out-of-sample performance since it better mimics the application of the model to out-of-sample data.

------------------------------------------------------------------------------------------------------------------------------------
#https://github.com/Gurubux/StatQuest/blob/master/GridSearchCV_RandomizedSearchCV/08_grid_searchnew.ipynb
More efficient parameter tuning using GridSearchCV GridSearchCV()
