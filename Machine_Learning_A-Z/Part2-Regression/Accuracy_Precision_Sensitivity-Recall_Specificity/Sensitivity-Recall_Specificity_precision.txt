SENSITIVITY_SPECIFICITY

Sensitivity(Recall) - % of positive data correctly predicted - True positive rate
Specificity 		- % of negative data correctly predicted - True negative rate



When Confusion-Matrix has 2 n x m
				  PREDICTED
				   N    P
ACTUAL		N 	 [ TN   FP
			P      FN   TP ]
# https://raw.githubusercontent.com/Gurubux/Udemy-ML/master/Machine_Learning_A-Z/Part2-Regression/Accuracy_Precision_Sensitivity-Recall_Specificity/PrecisionAccuracySensitivitySpecificity_Using_ConfusionMatrix.jpg				   
SENSITIVITY-RECALL  = tp / t = tp / (tp + fn)
SPECIFICITY 		= tn / n = tn / (tn + fp)
PRECISION 			= tp / p = tp / (tp + fp)
ACCURACY 			= tp + tn / (Total)

- Sensitivity/recall 	– how good a test is at detecting the positives. A test can cheat and maximize this by always returning “positive”.
- Specificity 			– how good a test is at avoiding false alarms. A test can cheat and maximize this by always returning “negative”.
- PRECISION 			– how many of the positively classified were relevant. A test can cheat and maximize this by only returning positive on one result it’s most confident in.
- ACCURACY 				– Correct Predictions

The cheating is resolved by looking at both relevant metrics instead of just one. E.g. the cheating 100% sensitivity that always says “positive” has 0% specificity.

-----------------------------------------
y_true = [0, 0, 0, 1, 1, 1, 1, 1]
y_pred = [0, 1, 0, 1, 0, 1, 0, 1]
tn, fp, fn, tp = confusion_matrix(y_true, y_pred).ravel()
print(confusion_matrix(y_true, y_pred))
print('tn, fp, fn, tp',tn, fp, fn, tp)


cm1 = confusion_matrix(y_true, y_pred)

total1=sum(sum(cm1))

accuracy=(cm1[0,0]+cm1[1,1])/total1
print ('Accuracy : ', accuracy1)

sensitivity = cm1[1,1]/(cm1[1,1]+cm1[1,0])
print('Sensitivity : ', sensitivity1 )

specificity= cm1[0,0]/(cm1[0,0]+cm1[0,1])
print('Specificity : ', specificity1)

precision = cm1[1,1]/(cm1[0,1]+cm1[1,1])
print('Precision : ', precision)
-----------------------------------------
>>>
[[2 1]
 [2 3]]
tn, fp, fn, tp 2 1 2 3
Accuracy :  0.2
Sensitivity :  1.0
Specificity :  0.0
Precision :  0.75


y_true = [0, 0, 0, 1, 1, 1, 1, 1]
y_pred = [0, 1, 0, 1, 0, 1, 0, 1]
tn, fp, fn, tp = confusion_matrix(y_true, y_pred).ravel()
print(confusion_matrix(y_true, y_pred))
print('tn, fp, fn, tp',tn, fp, fn, tp)


cm1 = confusion_matrix(y_true, y_pred)

total1=sum(sum(cm1))

accuracy=(cm1[0,0]+cm1[1,1])/total1
print ('Accuracy : ', accuracy)

sensitivity = cm1[1,1]/(cm1[1,1]+cm1[1,0])
print('Sensitivity : ', sensitivity )

specificity= cm1[0,0]/(cm1[0,0]+cm1[0,1])
print('Specificity : ', specificity)

precision = cm1[1,1]/(cm1[0,1]+cm1[1,1])
print('Precision : ', precision)

plot_confusion_matrix(y_true, y_pred, classes=np.array([0,1]), title='Confusion matrix, without normalization')







When Confusion-Matrix has 3 or more n x m
https://youtu.be/sunUKFXMHGk?t=332