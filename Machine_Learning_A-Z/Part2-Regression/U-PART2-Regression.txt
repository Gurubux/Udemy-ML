--------------------------------------------------------------------------------------------------
PART 2 
SECTION 3 - REGRESSION
--------------------------------------------------------------------------------------------------
21. Welcome to Part 2 - Regression!
Regression models (both linear and non-linear) are used for predicting a real value, like salary for example. If your independent variable is time, then you are forecasting future values, otherwise your model is predicting present but unknown values. Regression technique vary from Linear Regression to SVR and Random Forests Regression.
	Simple Linear Regression
	Multiple Linear Regression
	Polynomial Regression
	Support Vector for Regression (SVR)
	Decision Tree Classification
	Random Forest Classification
--------------------------------------------------------------------------------------------------
SECTION 4 - Simple Linear Regression
--------------------------------------------------------------------------------------------------
    ___________________
   / y = b₀ + b₁X₁    /
  /__________________/


Code 	- simple_linear_regression.py  
DATASET - Salary Data
GOAL 	- To predict SALARY based on YearsExperience

--------------------------------------------------------------------------------------------------
SECTION 5 - Multiple Linear Regression 
--------------------------------------------------------------------------------------------------
Linear Model
    ____________________________________
   / y = b₀ + b₁X₁ + b₂X₂ .... bₙXₙ    /
  /__________________________________/
Code    - multiple_linear_regression_backwordElimination.py  
DATASET - 50 Startups
GOAL    - To predict profits based on multiple factors(Independent variables)

Step 1 : Consider the Assumptions that take place in any Linear Regression model

Step 2 : If any Categorical Variable(Non-Numeric Column), create dummy variables for the same
		 (Note : Should not include all of your dummy variable columns, use N-1 dummy variables only. So if 3 Variables i.e NY, CA,LA then use any 2 only. (DUMMY VARIABLE TRAP))
Step 3 : Perform Backword Elimination for building the model with appropriate variables
		 Refer - https://raw.githubusercontent.com/Gurubux/Udemy-ML/master/Machine_Learning_A-Z/Part2-Regression/Section5-Multiple_Linear_Regression/Homework_Solutions/multiple_linear_regression_backwordElimination.py
Step 3 : Fit
Step 4 : Predict

WhiteBoard - Refer - https://raw.githubusercontent.com/Gurubux/Udemy-ML/master/Machine_Learning_A-Z/Part2-Regression/Section5-Multiple_Linear_Regression/BackwardElimination.jpg

Extra - 
BACKWARD ELIMINATION WITH P-VALUES ONLY:
BACKWARD ELIMINATION WITH P-VALUES AND ADJUSTED R SQUARED:
https://raw.githubusercontent.com/Gurubux/Udemy-ML/master/Machine_Learning_A-Z/Part2-Regression/Section5-Multiple_Linear_Regression/BACKWARD_ELIMINATION_WITH_P-VALUES_AND_ADJUSTED_R_SQUARED.txt


--------------------------------------------------------------------------------------------------
SECTION 6 - Polynomial Regression ⁰¹²³ⁿ ₀₁₂₃ₙ
--------------------------------------------------------------------------------------------------
 Non-Linear Model Regressors
   ___________________________________
   / y = b₀ + b₁X₁ + b₂X₁² .... bₙX₁ⁿ /
  /__________________________________/



EXAMPLES : 
Used to describe how diseases spread or pandemics and epidemics spred across territory

Code    - PRACTICE_2_6_PolyR.py  
DATASET - Position_Salaries.csv
GOAL    - To predict whether the new Employee is bluffing or not about his salary - Truth/Bluff Detector

PolynomialFeatures(degree = 4)


--------------------------------------------------------------------------------------------------
SECTION 7 - Support Vector for Regression (SVR)
--------------------------------------------------------------------------------------------------
Non-Linear Model
refer SupportVectorRegressionModel_SVR.docx (https://github.com/Gurubux/Udemy-ML/raw/master/Machine_Learning_A-Z/Part2-Regression/Section7-SupportVectorRegression-SVR/SupportVectorRegressionModel_SVR.docx)

Code    - svr.py  
DATASET - Position_Salaries.csv
GOAL    - To predict whether the new Employee is bluffing or not about his salary - Truth/Bluff Detector

Code    - PRACTICE-SVR.py
DATASET - numpy generated data
GOAL    - To observe 'RBF', 'Linear', 'Polynomial' kernels in SVR regrerssion

--------------------------------------------------------------------------------------------------
SECTION 8 - Decision Tree Classification
--------------------------------------------------------------------------------------------------
Non-Linear and Non Continous regression Model
Code    - svr.py  
DATASET - Position_Salaries.csv
GOAL    - To predict whether the new Employee is bluffing or not about his salary - Truth/Bluff Detector


CART - https://www.youtube.com/watch?v=nWuUahhK3Oc
CART - Splitting https://raw.githubusercontent.com/Gurubux/Udemy-ML/master/Machine_Learning_A-Z/Part2-Regression/Section8-DecisionTreeRegression/CART-Splitting.PNG
CART - Decision Tree https://raw.githubusercontent.com/Gurubux/Udemy-ML/master/Machine_Learning_A-Z/Part2-Regression/Section8-DecisionTreeRegression/CART-DT.PNG
https://raw.githubusercontent.com/Gurubux/Udemy-ML/master/Machine_Learning_A-Z/Part2-Regression/Section8-DecisionTreeRegression/CART-DT_Splitting.png


Divide and conquer strategy, to divide the training data into subsets that are pure and pure OR more and more homogenous 

1. SELECTING THE ROOT
    Will go through all the predictors and select one that is most predictive of the target feature. That will be the root of our tree.
    HOW ? by StdDev or Variance or sum of squares or absolute deviation are ways to measure the purity of resulting subgroups.
    Using STD Deviation. 
        a. Calculate the STD of the Target. STD(exam)
        b. Calculate STD of the Target varable for each unique value of each independent variable.
            STD(exam | tutorials = all)
            STD(exam | tutorials = some)
            STD(exam | labs = complete)
            STD(exam | labs = partial)
        c. Calculate COUNT of the Target varable for each unique value of each independent variable.
            COUNT(exam | tutorials = all)
            COUNT(exam | tutorials = some)
            COUNT(exam | labs = complete)
            COUNT(exam | labs = partial)
        d. Calculate the WEIGHTED STD for each independent variable.
            WEIGHTED STD(tutorials) = [ STD(exam | tutorials = all) * COUNT(exam | tutorials = all) / 15 ] + [ STD(exam | tutorials = some) * COUNT(exam | tutorials = some) / 15 ]
            WEIGHTED STD(labs) = [ STD(exam | labs = complete) * COUNT(exam | labs = complete) / 15 ] + [ STD(exam | labs = partial) * COUNT(exam | labs = partial) / 15 ]
        e. Calculate STDReduction of each independent variable.
           STDReduction(tutorials)   =  STD(exam) - WEIGHTED STD(tutorials)
           STDReduction(labs)        =  STD(exam) - WEIGHTED STD(labs)

        The greatest of each independent variable's STDReduction will be more prominent in predicting the Target variable and be chosen as the ROOT

2. PREDICTION
             | ROOT |                                         |     labs    |
          x /       \ y                   --->      complete /               \ partial
           /         \                                      /                 \
AVG(TV | ROOT = x) AVG(TV | ROOT = y)                      /                   \
                                                          /                     \
                                       AVG(exam | labs = complete)            AVG(exam | labs = partial)


Also DT Regression does the splitting(criterion) based on "MSE" or "MAE" or "FRIEDMAN_MSE"

Also DT Classifier does the splitting(criterion) based on "Entropy" for Information Gain OR 'gini' for gini impurities



--------------------------------------------------------------------------------------------------
SECTION 9 - Random Forest Classification
--------------------------------------------------------------------------------------------------
Non-Linear and Non Continous regression Model

Bias-Variance Trade Off
Ensemble - Bagging
Random Forest 

--------------------------------------------------------------------------------------------------
SECTION 10 - Evaluating Regression Models Performance
--------------------------------------------------------------------------------------------------

--------------------------------------------------------------------------------------------------
SECTION 11 - Regularization Methods
--------------------------------------------------------------------------------------------------
Lasso
Ridge
ElasticNet
--------------------------------------------------------------------------------------------------
SECTION 12 - Part Recap
--------------------------------------------------------------------------------------------------
