--------------------------------------------------------------------------------------------------
PART 2 
SECTION 3 - REGRESSION
--------------------------------------------------------------------------------------------------
21. Welcome to Part 2 - Regression!
Regression models (both linear and non-linear) are used for predicting a real value, like salary for example. If your independent variable is time, then you are forecasting future values, otherwise your model is predicting present but unknown values. Regression technique vary from Linear Regression to SVR and Random Forests Regression.
	Simple Linear Regression
	Multiple Linear Regression
	Polynomial Regression
	Support Vector for Regression (SVR)
	Decision Tree Classification
	Random Forest Classification
--------------------------------------------------------------------------------------------------
SECTION 4 - Simple Linear Regression
--------------------------------------------------------------------------------------------------
    ___________________
   / y = b₀ + b₁X₁    /
  /__________________/


Code 	- simple_linear_regression.py  
DATASET - Salary Data
GOAL 	- To predict SALARY based on YearsExperience

--------------------------------------------------------------------------------------------------
SECTION 5 - Multiple Linear Regression 
--------------------------------------------------------------------------------------------------
Linear Model
    ____________________________________
   / y = b₀ + b₁X₁ + b₂X₂ .... bₙXₙ    /
  /__________________________________/
Code    - multiple_linear_regression_backwordElimination.py  
DATASET - 50 Startups
GOAL    - To predict profits based on multiple factors(Independent variables)

Step 1 : Consider the Assumptions that take place in any Linear Regression model

Step 2 : If any Categorical Variable(Non-Numeric Column), create dummy variables for the same
		 (Note : Should not include all of your dummy variable columns, use N-1 dummy variables only. So if 3 Variables i.e NY, CA,LA then use any 2 only. (DUMMY VARIABLE TRAP))
Step 3 : Perform Backword Elimination for building the model with appropriate variables
		 Refer - https://raw.githubusercontent.com/Gurubux/Udemy-ML/master/Machine_Learning_A-Z/Part2-Regression/Section5-Multiple_Linear_Regression/Homework_Solutions/multiple_linear_regression_backwordElimination.py
Step 3 : Fit
Step 4 : Predict

WhiteBoard - Refer - https://raw.githubusercontent.com/Gurubux/Udemy-ML/master/Machine_Learning_A-Z/Part2-Regression/Section5-Multiple_Linear_Regression/BackwardElimination.jpg

Extra - 
BACKWARD ELIMINATION WITH P-VALUES ONLY:
BACKWARD ELIMINATION WITH P-VALUES AND ADJUSTED R SQUARED:
https://raw.githubusercontent.com/Gurubux/Udemy-ML/master/Machine_Learning_A-Z/Part2-Regression/Section5-Multiple_Linear_Regression/BACKWARD_ELIMINATION_WITH_P-VALUES_AND_ADJUSTED_R_SQUARED.txt

"ChiSquareAndP-value"

"ProbabiltyInNormalDensityCurves"
--------------------------------------------------------------------------------------------------
SECTION 6 - Polynomial Regression ⁰¹²³ⁿ ₀₁₂₃ₙ
--------------------------------------------------------------------------------------------------
 Non-Linear Model Regressors
   ___________________________________
   / y = b₀ + b₁X₁ + b₂X₁² .... bₙX₁ⁿ /
  /__________________________________/



EXAMPLES : 
Used to describe how diseases spread or pandemics and epidemics spred across territory

Code    - PRACTICE_2_6_PolyR.py  
DATASET - Position_Salaries.csv
GOAL    - To predict whether the new Employee is bluffing or not about his salary - Truth/Bluff Detector

PolynomialFeatures(degree = 4)


--------------------------------------------------------------------------------------------------
SECTION 7 - Support Vector for Regression (SVR)
--------------------------------------------------------------------------------------------------
Non-Linear Model
refer SupportVectorRegressionModel_SVR.docx (https://github.com/Gurubux/Udemy-ML/raw/master/Machine_Learning_A-Z/Part2-Regression/Section7-SupportVectorRegression-SVR/SupportVectorRegressionModel_SVR.docx)

Code    - svr.py  
DATASET - Position_Salaries.csv
GOAL    - To predict whether the new Employee is bluffing or not about his salary - Truth/Bluff Detector

Code    - PRACTICE-SVR.py
DATASET - numpy generated data
GOAL    - To observe `RBF`, `Linear`, `Polynomial` kernels in SVR regrerssion

from sklearn.svm import SVR
regressor = SVR(kernel = 'rbf')

--------------------------------------------------------------------------------------------------
SECTION 8 - Decision Tree Classification
--------------------------------------------------------------------------------------------------
Non-Linear and Non Continous regression Model
Code    - svr.py  
DATASET - Position_Salaries.csv
GOAL    - To predict whether the new Employee is bluffing or not about his salary - Truth/Bluff Detector


CART - https://www.youtube.com/watch?v=nWuUahhK3Oc
CART - Splitting https://raw.githubusercontent.com/Gurubux/Udemy-ML/master/Machine_Learning_A-Z/Part2-Regression/Section8-DecisionTreeRegression/CART-Splitting.PNG
CART - Decision Tree https://raw.githubusercontent.com/Gurubux/Udemy-ML/master/Machine_Learning_A-Z/Part2-Regression/Section8-DecisionTreeRegression/CART-DT.PNG
https://raw.githubusercontent.com/Gurubux/Udemy-ML/master/Machine_Learning_A-Z/Part2-Regression/Section8-DecisionTreeRegression/CART-DT_Splitting.png


Divide and conquer strategy, to divide the training data into subsets that are pure and pure OR more and more homogenous 

https://www.youtube.com/watch?v=nWuUahhK3Oc
1. SELECTING THE ROOT
    Will go through all the predictors and select one that is most predictive of the target feature. That will be the root of our tree.
    HOW ? by StdDev or Variance or sum of squares or absolute deviation are ways to measure the purity of resulting subgroups.
    Using STD Deviation. 
        a. Calculate the STD of the Target. STD(exam)
        b. Calculate STD of the Target varable for each unique value of each independent variable.
            STD(exam | tutorials = all)
            STD(exam | tutorials = some)
            STD(exam | labs = complete)
            STD(exam | labs = partial)
        c. Calculate COUNT of the Target varable for each unique value of each independent variable.
            COUNT(exam | tutorials = all)
            COUNT(exam | tutorials = some)
            COUNT(exam | labs = complete)
            COUNT(exam | labs = partial)
        d. Calculate the WEIGHTED STD for each independent variable.
            WEIGHTED STD(tutorials) = [ STD(exam | tutorials = all) * COUNT(exam | tutorials = all) / 15 ] + [ STD(exam | tutorials = some) * COUNT(exam | tutorials = some) / 15 ]
            WEIGHTED STD(labs) = [ STD(exam | labs = complete) * COUNT(exam | labs = complete) / 15 ] + [ STD(exam | labs = partial) * COUNT(exam | labs = partial) / 15 ]
        e. Calculate STDReduction of each independent variable.
           STDReduction(tutorials)   =  STD(exam) - WEIGHTED STD(tutorials)
           STDReduction(labs)        =  STD(exam) - WEIGHTED STD(labs)

        The GREATEST of each independent variable`s STDReduction will be more prominent in predicting the Target variable and be chosen as the ROOT

2. PREDICTION
             | ROOT |                                         |     labs    |
          x /       \ y                   --->      complete /               \ partial
           /         \                                      /                 \
AVG(TgtV | ROOT = x) AVG(TgtV | ROOT = y)                  /                   \
                                                          /                     \
                                       AVG(exam | labs = complete)            AVG(exam | labs = partial)

3. If we have a third variable for the second node then do step 1 and 2 again https://www.youtube.com/watch?v=nSaOuPCNvlk


Also DT Regression does the splitting(criterion) based on "MSE" or "MAE" or "FRIEDMAN_MSE"

Also DT Classifier does the splitting(criterion) based on "Entropy" for Information Gain OR `gini` for gini impurities


from sklearn.tree import DecisionTreeRegressor #  DecisionTreeRegressor(self, criterion="mse", splitter="best", max_depth=None, min_samples_split=2, min_samples_leaf=1, min_weight_fraction_leaf=0., max_features=None, random_state=None, max_leaf_nodes=None, min_impurity_decrease=0., min_impurity_split=None, presort=False)
regressor = DecisionTreeRegressor(random_state = 0)

--------------------------------------------------------------------------------------------------
SECTION 9 - Random Forest Classification
--------------------------------------------------------------------------------------------------
Non-Linear and Non Continous regression Model

Code    - random_forest_regression.py 
DATASET - Position_Salaries.csv
GOAL    - To predict whether the new Employee is bluffing or not about his salary - Truth/Bluff Detector


Bias-Variance Trade Off
Ensemble - Bagging, Boosting and regularization
Random Forest 

Bias-Variance     - https://raw.githubusercontent.com/Gurubux/Udemy-ML/master/Machine_Learning_A-Z/Part2-Regression/Section9-RandomForestRegression/Bias-Variance.txt

Ensemble Learning - When you take multiple algorithms or the same algorithm multiple times and you put them together to make something more powerful then the orginal

Steps to perform  - https://raw.githubusercontent.com/Gurubux/Udemy-ML/master/Machine_Learning_A-Z/Part2-Regression/Section9-RandomForestRegression/Steps_RF.PNG

Random Forest  
from sklearn.ensemble import RandomForestRegressor
regressor = RandomForestRegressor(n_estimators = 10, random_state = 0)
--------------------------------------------------------------------------------------------------
SECTION 10 - Evaluating Regression Models Performance
--------------------------------------------------------------------------------------------------
- R-Squared Intuition
- Adjusted R-Squared Intuition
- Evaluating Regression Models Performance - Homework`s Final Part
- Interpreting Linear Regression Coefficients

- R² Intuition
    R² Score  = 1 - RSS/TSS  =  1 - RSE
- Adjusted R² Intuition
    Adjusted R² has a penalization factor. It penalizes you for adding independent variables that don`t help your model.
                                ( n - 1 )
    Adj R²    = 1 - (1 - R²) -----------------
                              ( n - p - 1 )


                                ( n - 1 )
              = 1 - (RSS/TSS) -----------------
                              ( n - p - 1 )

                Where,
                    p = number of Regressors
                    n = sample size

    So as seen from the formula as the number of regressor(independent variable) increases the Adj R² decreases thus penalizing.
    It`s a very good metric it helps you understand whether YOU`RE ADDING GOOD VARIABLES TO MODEL OR NOT.


    Example :  https://raw.githubusercontent.com/Gurubux/Udemy-ML/master/Machine_Learning_A-Z/Part2-Regression/Section5-Multiple_Linear_Regression/Homework_Solutions/multiple_linear_regression_backwordElimination.py
        In backward elimination step Rsquared for [0, 3, 5] was 0.947 
                                          and for [0, 3] it was 0.944 that means by removing a variable it decreased the Rsquared value
                                          thus [0, 3, 5] are better predictors of dependent variable y



1. WHAT ARE THE PROS AND CONS OF EACH MODEL ?
    https://github.com/Gurubux/Udemy-ML/blob/master/Machine_Learning_A-Z/Part2-Regression/Section10-EvaluatingRegressionModelsPerformance/P14-Regression-Pros-Cons.pdf
2. HOW DO I KNOW WHICH MODEL TO CHOOSE FOR MY PROBLEM ?    
    a. First, you need to figure out whether your problem is linear or non linear. You will learn how to do that in Part 10 - Model Selection.
    b. If your problem is linear, you should go for SIMPLE LINEAR REGRESSION if you only have one feature, and MULTIPLE LINEAR REGRESSION if you have several features.
    c. If your problem is non linear, you should go for POLYNOMIAL REGRESSION, SVR, DECISION TREE OR RANDOM FOREST. 
    d. Then which one should you choose among these four ? That you will learn in Part 10 - Model Selection. 
3. HOW CAN I IMPROVE EACH OF THESE MODELS ?
    Parameter Tuning - Part 10 - Model Selection.
    You probably already noticed that each model is composed of two types of parameters:
        a. the parameters that are learnt, for example the coefficients in Linear Regression,
        b. the hyperparameters.
            The hyperparameters are the parameters that are not learnt and that are fixed values inside the model equations. 
            For example, the regularization parameter lambda or the penalty parameter C are hyperparameters. So far we used the default value 
            of these hyperparameters, and we haven`t searched for their optimal value so that your model reaches even higher performance. 
            Finding their optimal value is exactly what Parameter Tuning is about. So for those of you already interested in improving your 
            model performance and doing some parameter tuning, feel free to jump directly to Part 10 - Model Selection.





--------------------------------------------------------------------------------------------------
SECTION 11 - Regularization Methods
--------------------------------------------------------------------------------------------------
Lasso
Ridge
ElasticNet
https://sds-platform-private.s3-us-east-2.amazonaws.com/uploads/P14-Regularization.pdf

--------------------------------------------------------------------------------------------------
SECTION 12 - Part Recap
--------------------------------------------------------------------------------------------------



https://raw.githubusercontent.com/Gurubux/Udemy-ML/master/Machine_Learning_A-Z/Part2-Regression/NonLinear-Regression-Poly%2CSVR%2CDecisionTree%2CRandomForest.jpg