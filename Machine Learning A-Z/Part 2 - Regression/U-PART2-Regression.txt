--------------------------------------------------------------------------------------------------
PART 2 
SECTION 3 - REGRESSION
--------------------------------------------------------------------------------------------------
21. Welcome to Part 2 - Regression!
Regression models (both linear and non-linear) are used for predicting a real value, like salary for example. If your independent variable is time, then you are forecasting future values, otherwise your model is predicting present but unknown values. Regression technique vary from Linear Regression to SVR and Random Forests Regression.
	Simple Linear Regression
	Multiple Linear Regression
	Polynomial Regression
	Support Vector for Regression (SVR)
	Decision Tree Classification
	Random Forest Classification
--------------------------------------------------------------------------------------------------
SECTION 4 - Simple Linear Regression
--------------------------------------------------------------------------------------------------
Code 	- simple_linear_regression.py  
DATASET - Salary Data
GOAL 	- To predict SALARY based on YearsExperience

--------------------------------------------------------------------------------------------------
SECTION 5 - Multiple Linear Regression
--------------------------------------------------------------------------------------------------
Step 1 : Consider the Assumptions that take place in any Linear Regression model

Step 2 : If any Categorical Variable(Non-Numeric Column), create dummy variables for the same
		(Note : Should not include all of your dummy variable columns, use N-1 dummy variables only. So if 3 Variables i.e NY, CA,LA then use any 2 only. (DUMMY VARIABLE TRAP))
Step 3 : Perform Backword Elimination for building the model with appropriate variables
		Refer - https://raw.githubusercontent.com/Gurubux/Udemy-ML/master/Machine%20Learning%20A-Z/Part%202%20-%20Regression/Section%205%20-%20Multiple%20Linear%20Regression/Homework_Solutions/multiple_linear_regression_backwordElimination.py
Step 3 : Fit
Step 4 : Predict

Refer - https://raw.githubusercontent.com/Gurubux/Udemy-ML/master/Machine%20Learning%20A-Z/Part%202%20-%20Regression/Section%205%20-%20Multiple%20Linear%20Regression/BackwardElimination.jpg




BACKWARD ELIMINATION WITH P-VALUES ONLY:

import statsmodels.formula.api as sm
def backwardElimination(x, sl):
    numVars = len(x[0])
    for i in range(0, numVars):
        regressor_OLS = sm.OLS(y, x).fit()
        maxVar = max(regressor_OLS.pvalues).astype(float)
        if maxVar > sl:
            for j in range(0, numVars - i):
                if (regressor_OLS.pvalues[j].astype(float) == maxVar):
                    x = np.delete(x, j, 1)
    regressor_OLS.summary()
    return x
 
SL = 0.05
X_opt = X[:, [0, 1, 2, 3, 4, 5]]
X_Modeled = backwardElimination(X_opt, SL)




BACKWARD ELIMINATION WITH P-VALUES AND ADJUSTED R SQUARED:

import statsmodels.formula.api as sm
def backwardElimination(x, SL):
    numVars = len(x[0])
    temp = np.zeros((50,6)).astype(int)
    for i in range(0, numVars):
        regressor_OLS = sm.OLS(y, x).fit()
        maxVar = max(regressor_OLS.pvalues).astype(float)
        adjR_before = regressor_OLS.rsquared_adj.astype(float)
        if maxVar > SL:
            for j in range(0, numVars - i):
                if (regressor_OLS.pvalues[j].astype(float) == maxVar):
                    temp[:,j] = x[:, j]
                    x = np.delete(x, j, 1)
                    tmp_regressor = sm.OLS(y, x).fit()
                    adjR_after = tmp_regressor.rsquared_adj.astype(float)
                    if (adjR_before >= adjR_after):
                        x_rollback = np.hstack((x, temp[:,[0,j]]))
                        x_rollback = np.delete(x_rollback, j, 1)
                        print (regressor_OLS.summary())
                        return x_rollback
                    else:
                        continue
    regressor_OLS.summary()
    return x
 
SL = 0.05
X_opt = X[:, [0, 1, 2, 3, 4, 5]]
X_Modeled = backwardElimination(X_opt, SL)
--------------------------------------------------------------------------------------------------
SECTION 6 - Polynomial Regression
--------------------------------------------------------------------------------------------------
--------------------------------------------------------------------------------------------------
SECTION 7 - Support Vector for Regression (SVR)
--------------------------------------------------------------------------------------------------
--------------------------------------------------------------------------------------------------
SECTION 8 - Decision Tree Classification
--------------------------------------------------------------------------------------------------
--------------------------------------------------------------------------------------------------
SECTION 9 - Random Forest Classification
--------------------------------------------------------------------------------------------------
--------------------------------------------------------------------------------------------------
SECTION 10 - Evaluating Regression Models Performance
--------------------------------------------------------------------------------------------------
--------------------------------------------------------------------------------------------------
SECTION 11 - Regularization Methods
--------------------------------------------------------------------------------------------------
--------------------------------------------------------------------------------------------------
SECTION 12 - Part Recap
--------------------------------------------------------------------------------------------------